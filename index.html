<!DOCTYPE html>
<html lang="en">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">

  <title> Navarrs</title>
  <link rel="shortcut icon" href="data/img/ai.png" type="image/png"/>

  <!-- Bootstrap core CSS -->
  <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Custom fonts for this template -->
  <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet">
  <link href="vendor/fontawesome-free/css/all.min.css" rel="stylesheet">
  <link href='https://fonts.googleapis.com/css?family=Nunito' rel='stylesheet'>

  <!-- Custom styles for this template -->
  <link href="css/resume.css" rel="stylesheet">
  <link href="css/project.css" rel="stylesheet">

</head>

<script>
  function show_abstract(x) {
    var x = document.getElementById(x);
    if (x.style.display === "none") {
      x.style.display = "block";
    } else {
      x.style.display = "none";
    }
  }
</script>

<body id="page-top">
  <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
    <a class="navbar-brand js-scroll-trigger" href="#page-top">
      <span class="d-block d-lg-none">Navars</span>
      <span class="d-none d-lg-block">
        <!-- <img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="data/img/profile.jpg" alt=""> -->
      </span>
    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav">
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#about">About</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#publications">Publications</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#resume" onclick="window.open('data/files/navars.pdf')">CV</a>
        </li>
      </ul>
    </div>
  </nav>

  <div class="container-fluid p-0">
    <section class="resume-section p-3 p-lg-5 d-flex align-items-center" id="about">
      <div class="w-100">
        <img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="data/img/profile.jpg" width="30%" alt="">
        <h2 class="mb-0">Ingrid 
          <span class="text-primary">Navarro</span>
        </h2>
        <p class="lead mb-5"> 
        I received a Masters in Robotics from Carnegie Mellon University (CMU) in 2022, and I'm currently 
        pursuing my PhD in Robotics also at CMU. I'm advised by <a href="https://www.cs.cmu.edu/~./jeanoh/">Jean Oh</a>, 
        head of the <a href="https://www.cs.cmu.edu/~./jeanoh/big/">roBot Intelligence Group (BIG)</a>. 
        </p>
        <p class="lead mb-5">
          My research interests lie under the umbrella of social robot navigation and multimodal embodied 
          navigation. I've worked on socially-aware motion prediction and navigation in multi-agent environments 
          such as pedestrian settings, urban driving, ground traffic in airports and navigation in airspace. 
          I've also worked on Vison-and-Language Navigation (VLN) and Audio-Visual Navigation (AVN), both 
          for indoor navigation. 
        </p>

        <!-- <p class="lead mb-5">
          I've work experience in computer vision, autonomous mobile robots and robot manipulators 
          for medical robotics.
        </p>

        <p class="lead mb-5"> 
        As an undergrad, I participated twice at the <a href="https://riss.ri.cmu.edu">Robotics Institute 
          Summer Scholars (RISS) </a> program, also at CMU, where I joined the Navigation Laboratory 
          (Navlab) and did research on <a href="https://riss.ri.cmu.edu/wp-content/uploads/2017/09/2017-RISS-Poster-KWAN-Kevin-NAVARRO-Ingrid.pdf">object detection systems (2017)</a> 
        and <a href="https://riss.ri.cmu.edu/wp-content/uploads/2018/11/2018-RISS-Poster-NAVARRO-Ingrid.pdf"> semantic segmentation of 3D LiDAR point 
          clouds (2018)</a>.
        </p> -->

        <div class="social-icons">
          <a href="https://www.linkedin.com/in/ingridnavarroan/">
            <i class="fab fa-linkedin-in"></i>
          </a>
          <a href="https://github.com/navarrs">
            <i class="fab fa-github"></i>
          </a>
          <a href="#resume" onclick="window.open('data/files/navars.pdf')">
            <p class="mb-0">CV</p>
          </a>
        </div>
      </div>
    </section>

    <hr class="m-0">

    <section class="resume-section p-3 p-lg-5 d-flex align-items-center" id="publications">
      
      <div class="container">

        <h2 class="mb-3">Publications</h2>
        
        <div class="grid-wrapper">
          <div class="profile center" style="grid-column: span 2; margin:auto;">
            <a href="https://arxiv.org/pdf/2304.01428.pdf">
              <img src="data/img/publications/sorts.png">
            </a>
          </div>
          <div style="grid-column: span 3;">
            <h4 class="mb-0">Learned Tree Search for Long-Horizon Social Robot Navigation in Shared Airspace</h4>
            <p class="blue" style="margin-top:0px; margin-bottom:0px;"> 
              <b class="text-primary">Ingrid Navarro*</b>, Jay Patrikar*, Joao P. A. Dantas, Rohan Baijal, Ian Higgins, Sebastian Scherer, and Jean Oh
            </p>
            <p style="margin-top:0px;"><i>Preprint in ArXiv (Under review). </i></p>
          
            <div class="center">
              <p>
                [<a class="link" href="https://arxiv.org/pdf/2304.01428.pdf">Paper</a> | 
                <a class="link" href="https://github.com/cmubig/sorts">Code</a> | 
                <a class="link" href="https://youtu.be/PBE3O4cW2rI">Video</a> ]
              </p>
            </div>

            <div onclick="show_abstract('paper_sorts')">
              <b class="abstract">Toggle Abstract</b>
            </div>
            
            <div id="paper_sorts" style="display:none;">
              <p class="lead mb-5" align="justify">
                The fast-growing demand for fully autonomous
                aerial operations in shared spaces necessitates developing
                trustworthy agents that can safely and seamlessly navigate in
                crowded, dynamic spaces. In this work, we propose <i>Social Robot
                Tree Search (SoRTS)</i>, an algorithm for the safe navigation
                of mobile robots in social domains. SoRTS aims to augment
                existing socially-aware trajectory prediction policies with a
                Monte Carlo Tree Search planner for improved downstream
                navigation of mobile robots. To evaluate the performance of
                our method, we choose the use case of social navigation for
                general aviation. To aid this evaluation, within this work, we
                also introduce X-PlaneROS, a high-fidelity aerial simulator,
                to enable more research in full-scale aerial autonomy. By
                conducting a user study based on the assessments of 26 FAA certified pilots, 
                we show that SoRTS performs comparably
                to a competent human pilot, significantly outperforming our
                baseline algorithm. We further complement these results with
                self-play experiments, showcasing our algorithm’s behavior in
                scenarios with increasing complexity.
                </p>
            </div>
          </div>
        </div>

        <hr class="small-rule" />

        <div class="grid-wrapper">
          <div class="profile center" style="grid-column: span 2; margin:auto;">
            <a href="https://arxiv.org/pdf/2212.11345.pdf">
              <img src="data/img/publications/ksaven.gif">
            </a>
          </div>
          <div style="grid-column: span 3;">
            <h4 class="mb-0">Knowledge-driven Scene Priors for Semantic Audio-Visual Embodied Navigation</h4>
            <p class="blue" style="margin-top:0px; margin-bottom:0px;"> 
              Gyan Tatiya, Jonathan Francis, Luca Bondi, <b class="text-primary">Ingrid Navarro</b>, Eric Nyberg, Jivko Sinapov, and Jean Oh
            </p>
            <p style="margin-top:0px;"><i>In ArXiv, 2022. </i></p>
          
            <div class="center">
              <p>
                [<a class="link" href="https://arxiv.org/pdf/2212.11345.pdf">Paper</a>]
              </p>
            </div>

            <div onclick="show_abstract('paper_ksven')">
              <b class="abstract">Toggle Abstract</b>
            </div>
            
            <div id="paper_ksven" style="display:none;">
              <p class="lead mb-5" align="justify">
                Generalisation to unseen contexts remains a challenge for embodied navigation agents. 
                In the context of semantic audio-visual navigation (SAVi) tasks, the notion of generalisation should include both generalising 
                to unseen indoor visual scenes as well as generalising to unheard sounding objects. However, previous SAVi task definitions 
                do not include evaluation conditions on truly novel sounding objects, resorting instead to evaluating agents on unheard sound 
                clips of known objects; meanwhile, previous SAVi methods do not include explicit mechanisms for incorporating domain knowledge 
                about object and region semantics. These weaknesses limit the development and assessment of models' abilities to generalise their 
                learned experience. In this work, we introduce the use of knowledge-driven scene priors in the semantic audio-visual embodied 
                navigation task: we combine semantic information from our novel knowledge graph that encodes object-region relations, spatial 
                knowledge from dual Graph Encoder Networks, and background knowledge from a series of pre-training tasks -- all within a 
                reinforcement learning framework for audio-visual navigation. We also define a new audio-visual navigation sub-task, where 
                agents are evaluated on novel sounding objects, as opposed to unheard clips of known objects. We show improvements over strong 
                baselines in generalisation to unseen regions and novel sounding objects, within the Habitat-Matterport3D simulation environment, 
                under the SoundSpaces task.
                </p>
            </div>
          </div>
        </div>

        <hr class="small-rule" />

        <div class="grid-wrapper">
          <div class="profile center" style="grid-column: span 2; margin:auto;">
            <a href="https://arxiv.org/pdf/2211.06932.pdf">
              <img src="data/img/publications/chuav.png">
            </a>
          </div>
          <div style="grid-column: span 3;">
            <h4 class="mb-0">Challenges in Close-Proximity Safe and Seamless Operation of Manned and Unmanned Aircraft in Shared Airspace</h4>
            <p class="blue" style="margin-top:0px; margin-bottom:0px;"> 
              Jay Patrikar, Joao P. A. Dantas, Sourish Ghosh, Parv Kapoor, Ian Higgins, Jasmine J. Aloor, <b class="text-primary">Ingrid Navarro</b>, Jimin Sun, Ben Stoler, Milad Hamidi, Rohan Baijal, Brady Moon, Jean Oh, Sebastian Scherer
            </p>
            <p style="margin-top:0px;"><i>In Aerial Robotics Workshop at the International Conference on Robotics and Automation (ICRA), 2022. </i></p>
          
            <div class="center">
              <p>
                [<a class="link" href="https://arxiv.org/pdf/2211.06932.pdf">Paper</a>]
              </p>
            </div>

            <div onclick="show_abstract('paper_chuav')">
              <b class="abstract">Toggle Abstract</b>
            </div>
            
            <div id="paper_chuav" style="display:none;">
              <p class="lead mb-5" align="justify">
                We propose developing an integrated system to keep autonomous unmanned aircraft safely separated and behave as expected in conjunction with 
                manned traffic. The main goal is to achieve safe manned-unmanned vehicle teaming to improve system performance, have each (robot/human) 
                teammate learn from each other in various aircraft operations, and reduce the manning needs of manned aircraft. The proposed system anticipates 
                and reacts to other aircraft using natural language instructions and can serve as a co-pilot or operate entirely autonomously. We point out the 
                main technical challenges where improvements on current state-of-the-art are needed to enable Visual Flight Rules to fully autonomous aerial operations, 
                bringing insights to these critical areas. Furthermore, we present an interactive demonstration in a prototypical scenario with one AI pilot 
                and one human pilot sharing the same terminal airspace, interacting with each other using language, and landing safely on the same runway.
                We also show a demonstration of a vision-only aircraft detection system.
                </p>
            </div>
          </div>
        </div>

        <hr class="small-rule" />

        <div class="grid-wrapper">
          <div class="profile center" style="grid-column: span 2; margin:auto;">
            <a href="https://www.ri.cmu.edu/app/uploads/2022/08/MSR_Thesis.pdf">
              <img src="data/img/publications/sprnn.gif">
            </a>
          </div>
          <div style="grid-column: span 3;">
            <h4 class="mb-0">Social-PatteRNN: Socially-Aware Trajectory Prediction Guided by Motion Patterns</h4>
            <p class="blue" style="margin-top:0px; margin-bottom:0px;"> 
              <b class="text-primary">Ingrid Navarro</b> and Jean Oh
            </p>
            <p style="margin-top:0px;"><i>In proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2022. </i></p>
          
            <div class="center">
              <p>
                [<a class="link" href="https://arxiv.org/pdf/2209.05649.pdf">Paper</a> | <a class="link" href="https://github.com/cmubig/social-patternn">Code</a>]
              </p>
            </div>

            <div onclick="show_abstract('paper_sprnn')">
              <b class="abstract">Toggle Abstract</b>
            </div>
            
            <div id="paper_sprnn" style="display:none;">
              <p class="lead mb-5" align="justify">
                As intelligent robots across domains start collaborating with humans in shared environments, 
                <i>e.g.</i>, urban settings and airspace, algorithms that enable them to reason over human motion 
                and intent are important to ensure seamless and safe interplay. Even beyond robotics, other domains,
                <i>e.g.</i>, surveillance and sports analysis, may also benefit from this type of algorithms.

                In our work, we study human intent by focusing on the problem of predicting trajectories in 
                dynamic environments. We are further interested in designing methods that are able to 
                generalize across domains. Specifically, we target domains where navigation guidelines 
                are relatively strictly defined yet not necessarily marked in their physical environments. 
                We hypothesize that within these domains, in the short-term, agents tend to exhibit motion 
                patterns that reveal important context information related to the agent's general direction,
                admissible motions, intermediate goals and social influences. From this intuition, we propose 
                <i>Social-PatteRNN</i>, a new recurrent generative model that exploits motion patterns to encode
                the aforesaid context information and use it as conditioning signal for predicting trajectories. 
                We assess our approach across three different problem domains: human motion in crowds, human 
                motion in sports and aircraft motion in terminal airspace. Finally, we show that our approach 
                achieves state-of-the-art results across these domains.
                </p>
            </div>
          </div>
        </div>

        <hr class="small-rule" />

        <div class="grid-wrapper">
          <div class="profile center" style="grid-column: span 2; margin:auto;">
            <a href="https://arxiv.org/abs/2106.13948">
              <img src="data/img/publications/evlp.png">
            </a>
          </div>
          <div style="grid-column: span 3;">
            <h4 class="mb-0">Core Challenges in Embodied Vision-Language Planning</h4>
            <p class="blue" style="margin-top:0px; margin-bottom:0px;">
              Jonathan Francis, Nariaki Kitamura, Felix Labelle, Xiaopen Lu, 
              <b class="text-primary">Ingrid Navarro</b> and Jean Oh
            </p>
            <p style="margin-top:0px;"><i>Journal of Artificial Intelligence Research (JAIR), 2022</i></p>
          
            <div class="center">
              <p>
                [<a class="link" href="https://jair.org/index.php/jair/article/view/13646/26807">Paper</a>]
              </p>
            </div>

            <div onclick="show_abstract('paper_evlp')">
              <b class="abstract">Toggle Abstract</b>
            </div>
            
            <div id="paper_evlp" style="display:none;">
              <p class="lead mb-5" align="justify">
                  Recent advances in the areas of multimodal machine learning and 
                  artificial intelligence (AI) have led to the development of challenging 
                  tasks at the intersection of Computer Vision, Natural Language Processing,
                    and Embodied AI. Whereas many approaches and previous survey pursuits 
                    have characterised one or two of these dimensions, there has not been 
                    a holistic analysis at the center of all three. Moreover, even when
                    combinations of these topics are considered, more focus is placed on 
                    describing, e.g., current architectural methods, as opposed to also 
                    illustrating high-level challenges and opportunities for the field. 
                    In this survey paper, we discuss Embodied Vision-Language Planning (EVLP) 
                    tasks, a family of prominent embodied navigation and manipulation problems 
                    that jointly use computer vision and natural language. We propose a taxonomy 
                    to unify these tasks and provide an in-depth analysis and comparison of the 
                    new and current algorithmic approaches, metrics, simulated environments, 
                    as well as the datasets used for EVLP tasks. Finally, we present the core 
                    challenges that we believe new EVLP works should seek to address, and
                    we advocate for task construction that enables model generalizability 
                    and furthers real-world deployment.
                </p>
            </div>
          </div>
        </div>

        <hr class="small-rule" />

        <div class="grid-wrapper">
          <div class="profile center" style="grid-column: span 2; margin:auto;">
            <a href="https://link.springer.com/chapter/10.1007%2F978-3-030-04497-8_28">
              <img src="data/img/publications/data-aug.png">
            </a>
          </div>
          <div style="grid-column: span 3;">
            <h4 class="mb-0">Data Augmentation in Deep Learning-Based Obstacle
               Detection System for Autonomous Navigation on Aquatic Surfaces</h4>
            <p class="blue" style="margin-top:0px; margin-bottom:0px;">
              <b class="text-primary">Ingrid Navarro</b>, Alberto Herrera, Itzel Hernández, and Leonardo Garrido 
            </p>
            <p style="margin-top:0px;"><i>Mexican International Conference on Artificial Intelligence (MICAI), 2018</i></p>
            <div class="center">
              <p>
                [ <a class="link" href="https://link.springer.com/chapter/10.1007%2F978-3-030-04497-8_28">Paper</a> |
                  <a class="link" href="https://github.com/navarrs/cv-roboboat">Code</a> 
              ]
              </p>
            </div>

            <div onclick="show_abstract('paper_vantec')">
              <b class="abstract">Toggle Abstract</b> 
            </div>
    
            <div id="paper_vantec" style="display:none;">
              <p align="justify">
                Deep learning-based frameworks have been widely used in
                object recognition, perception and autonomous navigation tasks, show-
                ing outstanding feature extraction capabilities. Nevertheless, the effec-
                tiveness of such detectors usually depends on large amounts of train-
                ing data. For specific object-recognition tasks, it is often difficult and
                time-consuming to gather enough valuable data [1]. Data Augmenta-
                tion has been broadly adopted to overcome these difficulties, as it allows
                to increase the training data and introduce variation in qualitative ele-
                ments like color, illumination, distortion and orientation. In this paper,
                we leverage on the object detection framework YOLOv2 [2] to evaluate
                the behavior of an obstacle detection system for an autonomous boat de-
                signed for the International RoboBoat Competition. We are focused on
                how the overall performance of a model changes with different augmen-
                tation techniques. Thus, we analyze the features that the network learns
                by using geometric and pixel-wise transformations to augment our data.
                Our instances of interest are buoys and sea markers, thus to generate
                training data comprising these classes, we simulated the aquatic surface
                of the boat and collected data from the COCO dataset [3]. Finally, we
                discuss that significant generalization is achieved in the learning process
                of our experiments using different augmentation techniques.
              </p>
            </div>
          </div>
        </div>
        
        <hr class="small-rule" />

        <div class="grid-wrapper">
          <div class="profile center" style="grid-column: span 2; margin:auto;">
            <a href="https://riss.ri.cmu.edu/wp-content/uploads/2018/11/RISS_Journal_Nov26-r.pdf">
              <img src="data/img/publications/sparseg.png">
            </a>
          </div>
          <div style="grid-column: span 3;">
            <h4 class="mb-0">Real-Time Semantic Segmentation of Sparse LiDAR Point Clouds using Lightweight CNN</h4>
            <p class="blue" style="margin-top:0px; margin-bottom:0px;">
              <b class="text-primary">Ingrid Navarro</b> and Luis Ernesto Navarro-Serment
            </p>
            <p style="margin-top:0px;"><i>RISS Working Papers Journal, vol. 6, 2018</i></p>
            <div class="center">
              <p>
                [ <a class="link" href="https://riss.ri.cmu.edu/wp-content/uploads/2018/11/RISS_Journal_Nov26-r.pdf">Paper</a> |
                  <a class="link" href="https://riss.ri.cmu.edu/wp-content/uploads/2018/11/2018-RISS-Poster-NAVARRO-Ingrid.pdf">Poster</a> |
                  <a class="link" href="https://github.com/navarrs/sparse-segmentation">Code</a>
              ]
              </p>
            </div>

            <div onclick="show_abstract('paper_sparseseg')">
              <b class="abstract">Toggle Abstract</b> 
            </div>
    
            <div id="paper_sparseseg" style="display:none;">
              <p align="justify">
                This paper proposes an approach to segment
                point clouds with high levels of vertical sparsity, which are
                typically generated by low-end LiDARs, like the Velodyne
                VLP-16. Special consideration is given–but not limited–to the
                identification of ground points. The approach addresses two
                important issues: the fact that the sparsity of points makes
                it hard to infer an object’s structure, and the difficulty of
                obtaining and annotating data for testing and training. The
                first issue is tackled by using a lightweight Convolutional
                Neural Network (CNN) with Recurrent CRF for point-wise
                class prediction. To address the second issue, which arises from
                the need to train this network, the proposed approach extracts
                sparse examples from dense point clouds available in public
                datasets. The approach was tested using down-sampled data
                from the KITTI dataset, which contains annotated examples
                of the object instances car, pedestrian and cyclist. However,
                since the ground class is often missing from annotated datasets,
                and given its significance in this work, a ground annotation
                algorithm was developed and used to automatically label the
                sparse data and add it to the other labeled classes available from
                the dataset. The sparse data obtained was used to train and test
                the CNN to characterize its prediction accuracy. Additionally,
                the network was tested using data collected with a low-end
                sensor. The experiments show that the system achieves accurate
                predictions in real time that are comparable to those reported
                from denser point clouds.
              </p>
            </div>

          </div>
        </div>

        <hr class="small-rule" />

        <div class="grid-wrapper">
          <div class="profile center" style="grid-column: span 2; ">
            <a href="https://riss.ri.cmu.edu/wp-content/uploads/2017/12/2017-RISS-Working-Papers-Journal-Final.pdf">
              <img src="data/img/publications/frcnn.png">
            </a>
          </div>
          <div style="grid-column: span 3;">
            <h4 class="mb-0">Faster RCNN-Based Wheelchair Detection System</h4>
            <p class="blue" style="margin-top:0px; margin-bottom:0px;">
              <b class="text-primary">Ingrid Navarro</b> and Luis Ernesto Navarro-Serment
            </p>
            <p style="margin-top:0px;"><i>RISS Working Papers Journal, vol. 5, 2017</i></p>
            <div class="center">
              <p>
                [ <a class="link" href="https://riss.ri.cmu.edu/wp-content/uploads/2017/12/2017-RISS-Working-Papers-Journal-Final.pdf">Paper</a> |
                  <a class="link" href="https://riss.ri.cmu.edu/wp-content/uploads/2017/09/2017-RISS-Poster-KWAN-Kevin-NAVARRO-Ingrid.pdf">Poster</a>
              ]
              </p>
            </div>
            
            <div onclick="show_abstract('paper_wheelchairs')">
              <b class="abstract">Toggle Abstract</b> 
            </div>
    
            <div id="paper_wheelchairs" style="display:none;">
                  <p align="justify">
                    Wheelchairs are one of the most important
                    auxiliary instruments for people with mobility impair-
                    ments. Accurate detection and tracking of these devices
                    could bring a number of improvements in automated
                    services that aim to assist, monitor and provide better
                    accessibility to allow wheelchair users to participate in
                    community life. In this paper, we present a Deep Learning-
                    based wheelchair detection and tracking system to address
                    some of the limitations of previous approaches, which
                    include detecting different types of wheelchairs in clut-
                    tered environments and from different viewing angles. We
                    explore region-based Convolutional Neural Networks (R-
                    CNN), in particular Faster R-CNN, as it has become one of
                    the top performers for object detection tasks. We evaluate
                    the performance of different training techniques using
                    two Faster R-CNN frameworks and different backbone
                    network structures. Furthermore, we present how we
                    empirically addressed some of the preceding limitations by
                    applying specific data augmentation techniques and con-
                    straints to our model. We demonstrate that using a region-
                    based implementation outperforms previous approaches in
                    terms of overall robustness, accuracy and flexibility.
                  </p>
                </div>
          </div>
        </div>

        <hr class="small-rule" />

      </div>
  </div>

    </section>
    
  </div>

  <!-- Bootstrap core JavaScript -->
  <script src="vendor/jquery/jquery.min.js"></script>
  <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

  <!-- Plugin JavaScript -->
  <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

  <!-- Custom scripts for this template -->
  <script src="js/resume.min.js"></script>

</body>

</html>
