<!DOCTYPE html>
<html lang="en">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">

  <title> Navarrs</title>
  <link rel="shortcut icon" href="data/img/ai.png" type="image/png"/>

  <!-- Bootstrap core CSS -->
  <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Custom fonts for this template -->
  <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet">
  <link href="vendor/fontawesome-free/css/all.min.css" rel="stylesheet">

  <!-- Custom styles for this template -->
  <link href="css/resume.css" rel="stylesheet">
  <link href="css/project.css" rel="stylesheet">

</head>

<body id="page-top">
  <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
    <a class="navbar-brand js-scroll-trigger" href="#page-top">
      <span class="d-block d-lg-none">Navars</span>
      <span class="d-none d-lg-block">
        <!-- <img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="data/img/profile.jpg" alt=""> -->
      </span>
    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav">
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#about">About</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#publications">Publications</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#projects">Projects</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#resume" onclick="window.open('data/files/navars.pdf')">CV</a>
        </li>
      </ul>
    </div>
  </nav>

  <div class="container-fluid p-0">
    <section class="resume-section p-3 p-lg-5 d-flex align-items-center" id="about">
      <div class="w-100">
        <img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="data/img/profile.jpg" width="30%" alt="">
        <h2 class="mb-0">Ingrid 
          <span class="text-primary">Navarro</span>
        </h2>
        <p class="lead mb-5"> 
        I'm a robotics graduate student at Carnegie Mellon University (CMU). 
        </p>
        <p class="lead mb-5">
        I have experience working in computer vision, autonomous mobile robots and robot manipulators for medical robotics.
        Some of my research interests include robot navigation and multimodal learning.
        I'm currently a member of the Bot Intelligence Group (BIG) at CMU doing research in Embodied AI and Social Robotics.
        </p>
        <p class="lead mb-5"> 
        As an undergrad, I participated twice at the <a href="https://riss.ri.cmu.edu">Robotics Institute Summer Scholars (RISS) </a> 
        program, also at CMU, where I joined the Navigation Laboratory (Navlab) and did research on 
        <a href="https://riss.ri.cmu.edu/wp-content/uploads/2017/09/2017-RISS-Poster-KWAN-Kevin-NAVARRO-Ingrid.pdf">object detection systems (2017)</a> 
        and <a href="https://riss.ri.cmu.edu/wp-content/uploads/2018/11/2018-RISS-Poster-NAVARRO-Ingrid.pdf"> semantic segmentation of 3D LiDAR point 
          clouds (2018)</a>.
        </p>
        <!-- <p class="lead mb-5"> 
        I was also part of <a href="https://www.vanttec.mx">VantTEC</a> where I participated on the design of an autonomous boat.    
        </p> -->
        <div class="social-icons">
          <a href="https://www.linkedin.com/in/ingridnavarroan/">
            <i class="fab fa-linkedin-in"></i>
          </a>
          <a href="https://github.com/navarrs">
            <i class="fab fa-github"></i>
          </a>
          <a href="#resume" onclick="window.open('data/files/navars.pdf')">
            <p class="mb-0">CV</p>
          </a>
        </div>
      </div>
    </section>

    <hr class="m-0">

    <section class="resume-section p-3 p-lg-5 d-flex align-items-center" id="publications">
      
      <div class="container">

        <h2 class="mb-3">Research Projects</h2>

        <div class="grid-wrapper">
          <div class="profile center" style="grid-column: span 2; margin:auto;">
            <a href="https://arxiv.org/abs/2106.13948">
              <img src="data/img/publications/evlp.png">
            </a>
          </div>
          <div style="grid-column: span 3;">
            <h3 class="mb-0">Core Challenges in Embodied Vision-Language Planning</h3>
            <p class="blue" style="margin-top:0px; margin-bottom:0px;">
              Jonathan Francis, Nariaki Kitamura, Felix Labelle, Xiaopen Lu, 
              <b class="text-primary">Ingrid Navarro</b> and Jean Oh
            </p>
            <p style="margin-top:0px;"><i>Preprint, Accepted for publication in the 
              Journal of Artificial Intelligence Research (JAIR), 2022</i></p>
            
            <div class="center">
              <p>
                [<a class="link" href="https://arxiv.org/abs/2106.13948">Paper</a>]
              </p>
            </div>
          </div>
        </div>

        <p><b class="text-primary">Abstract</b></p>
         <p class="lead mb-5" align="justify">
              Recent advances in the areas of multimodal machine learning and 
              artificial intelligence (AI) have led to the development of challenging 
              tasks at the intersection of Computer Vision, Natural Language Processing,
               and Embodied AI. Whereas many approaches and previous survey pursuits 
               have characterised one or two of these dimensions, there has not been 
               a holistic analysis at the center of all three. Moreover, even when
               combinations of these topics are considered, more focus is placed on 
               describing, e.g., current architectural methods, as opposed to also 
               illustrating high-level challenges and opportunities for the field. 
               In this survey paper, we discuss Embodied Vision-Language Planning (EVLP) 
               tasks, a family of prominent embodied navigation and manipulation problems 
               that jointly use computer vision and natural language. We propose a taxonomy 
               to unify these tasks and provide an in-depth analysis and comparison of the 
               new and current algorithmic approaches, metrics, simulated environments, 
               as well as the datasets used for EVLP tasks. Finally, we present the core 
               challenges that we believe new EVLP works should seek to address, and
                we advocate for task construction that enables model generalizability 
                and furthers real-world deployment.
            </p>

        <hr class="small-rule" />

        <div class="grid-wrapper">
          <div class="profile center" style="grid-column: span 2; margin:auto;">
            <a href="https://link.springer.com/chapter/10.1007%2F978-3-030-04497-8_28">
              <img src="data/img/publications/data-aug.png">
            </a>
          </div>
          <div style="grid-column: span 3;">
            <h3 class="mb-0">Data Augmentation in Deep Learning-Based Obstacle
               Detection System for Autonomous Navigation on Aquatic Surfaces</h3>
            <p class="blue" style="margin-top:0px; margin-bottom:0px;">
              <b class="text-primary">Ingrid Navarro</b>, Alberto Herrera, Itzel Hernández, and Leonardo Garrido 
            </p>
            <p style="margin-top:0px;"><i>Mexican International Conference on Artificial Intelligence (MICAI), 2018</i></p>
            <div class="center">
              <p>
                [ <a class="link" href="https://link.springer.com/chapter/10.1007%2F978-3-030-04497-8_28">Paper</a> |
                <a class="link" href="https://github.com/navarrs/cv-roboboat">Code</a>
              ]
              </p>
            </div>
          </div>
        </div>

        <p><b class="text-primary">Abstract</b></p>
            <p align="justify">
              Deep learning-based frameworks have been widely used in
              object recognition, perception and autonomous navigation tasks, show-
              ing outstanding feature extraction capabilities. Nevertheless, the effec-
              tiveness of such detectors usually depends on large amounts of train-
              ing data. For specific object-recognition tasks, it is often difficult and
              time-consuming to gather enough valuable data [1]. Data Augmenta-
              tion has been broadly adopted to overcome these difficulties, as it allows
              to increase the training data and introduce variation in qualitative ele-
              ments like color, illumination, distortion and orientation. In this paper,
              we leverage on the object detection framework YOLOv2 [2] to evaluate
              the behavior of an obstacle detection system for an autonomous boat de-
              signed for the International RoboBoat Competition. We are focused on
              how the overall performance of a model changes with different augmen-
              tation techniques. Thus, we analyze the features that the network learns
              by using geometric and pixel-wise transformations to augment our data.
              Our instances of interest are buoys and sea markers, thus to generate
              training data comprising these classes, we simulated the aquatic surface
              of the boat and collected data from the COCO dataset [3]. Finally, we
              discuss that significant generalization is achieved in the learning process
              of our experiments using different augmentation techniques.
            </p>

        <hr class="small-rule" />

        <div class="grid-wrapper">
          <div class="profile center" style="grid-column: span 2; margin:auto;">
            <a href="https://riss.ri.cmu.edu/wp-content/uploads/2018/11/RISS_Journal_Nov26-r.pdf">
              <img src="data/img/publications/sparseg.png">
            </a>
          </div>
          <div style="grid-column: span 3;">
            <h3 class="mb-0">Real-Time Semantic Segmentation of Sparse LiDAR Point Clouds using Lightweight CNN</h3>
            <p class="blue" style="margin-top:0px; margin-bottom:0px;">
              <b class="text-primary">Ingrid Navarro</b> and Luis Ernesto Navarro-Serment
            </p>
            <p style="margin-top:0px;"><i>RISS Working Papers Journal, vol. 6, 2018</i></p>
            <div class="center">
              <p>
                [ <a class="link" href="https://riss.ri.cmu.edu/wp-content/uploads/2018/11/RISS_Journal_Nov26-r.pdf">Paper</a> |
                  <a class="link" href="https://riss.ri.cmu.edu/wp-content/uploads/2018/11/2018-RISS-Poster-NAVARRO-Ingrid.pdf">Poster</a> |
                  <a class="link" href="https://github.com/navarrs/sparse-segmentation">Code</a>
              ]
              </p>
            </div>
          </div>
        </div>

        <p><b class="text-primary">Abstract</b></p>
            <p align="justify">
              This paper proposes an approach to segment
              point clouds with high levels of vertical sparsity, which are
              typically generated by low-end LiDARs, like the Velodyne
              VLP-16. Special consideration is given–but not limited–to the
              identification of ground points. The approach addresses two
              important issues: the fact that the sparsity of points makes
              it hard to infer an object’s structure, and the difficulty of
              obtaining and annotating data for testing and training. The
              first issue is tackled by using a lightweight Convolutional
              Neural Network (CNN) with Recurrent CRF for point-wise
              class prediction. To address the second issue, which arises from
              the need to train this network, the proposed approach extracts
              sparse examples from dense point clouds available in public
              datasets. The approach was tested using down-sampled data
              from the KITTI dataset, which contains annotated examples
              of the object instances car, pedestrian and cyclist. However,
              since the ground class is often missing from annotated datasets,
              and given its significance in this work, a ground annotation
              algorithm was developed and used to automatically label the
              sparse data and add it to the other labeled classes available from
              the dataset. The sparse data obtained was used to train and test
              the CNN to characterize its prediction accuracy. Additionally,
              the network was tested using data collected with a low-end
              sensor. The experiments show that the system achieves accurate
              predictions in real time that are comparable to those reported
              from denser point clouds.
            </p>

        <hr class="small-rule" />

        <div class="grid-wrapper">
          <div class="profile center" style="grid-column: span 2; margin:auto;">
            <a href="https://riss.ri.cmu.edu/wp-content/uploads/2017/12/2017-RISS-Working-Papers-Journal-Final.pdf">
              <img src="data/img/publications/frcnn.png">
            </a>
          </div>
          <div style="grid-column: span 3;">
            <h3 class="mb-0">Faster RCNN-Based Wheelchair Detection System</h3>
            <p class="blue" style="margin-top:0px; margin-bottom:0px;">
              <b class="text-primary">Ingrid Navarro</b> and Luis Ernesto Navarro-Serment
            </p>
            <p style="margin-top:0px;"><i>RISS Working Papers Journal, vol. 5, 2017</i></p>
            <div class="center">
              <p>
                [ <a class="link" href="https://riss.ri.cmu.edu/wp-content/uploads/2017/12/2017-RISS-Working-Papers-Journal-Final.pdf">Paper</a> |
                  <a class="link" href="https://riss.ri.cmu.edu/wp-content/uploads/2017/09/2017-RISS-Poster-KWAN-Kevin-NAVARRO-Ingrid.pdf">Poster</a>
              ]
              </p>
            </div>
          </div>
        </div>

        <p><b class="text-primary">Abstract</b></p>
            <p align="justify">
              Wheelchairs are one of the most important
              auxiliary instruments for people with mobility impair-
              ments. Accurate detection and tracking of these devices
              could bring a number of improvements in automated
              services that aim to assist, monitor and provide better
              accessibility to allow wheelchair users to participate in
              community life. In this paper, we present a Deep Learning-
              based wheelchair detection and tracking system to address
              some of the limitations of previous approaches, which
              include detecting different types of wheelchairs in clut-
              tered environments and from different viewing angles. We
              explore region-based Convolutional Neural Networks (R-
              CNN), in particular Faster R-CNN, as it has become one of
              the top performers for object detection tasks. We evaluate
              the performance of different training techniques using
              two Faster R-CNN frameworks and different backbone
              network structures. Furthermore, we present how we
              empirically addressed some of the preceding limitations by
              applying specific data augmentation techniques and con-
              straints to our model. We demonstrate that using a region-
              based implementation outperforms previous approaches in
              terms of overall robustness, accuracy and flexibility.
            </p>
        
            <hr class="small-rule" />

      </div>
    </div>

    </section>

    <section class="resume-section p-3 p-lg-5 d-flex align-items-center" id="projects">
      
      <div class="container">

        <h2 class="mb-3">Projects</h2>
        <div class="row">

        <div class="col-lg-4 mb-4">
        <div class="card">
          <img src="data/img/projects/sparseg.png" alt="" class="card-img-top">
          <div class="card-body">
            <h5 class="mb-0">
              SparseSeg 
              <a href="https://github.com/navarrs/sparse-segmentation"> <i class="fab fa-github"></i></a>
            </h5>
            <p class="card-text"> 
              Ground annotation tool for 3D Point Clouds
            </p>
          </div>
         </div>
        </div>

        <div class="col-lg-4 mb-4">
          <div class="card">
            <img src="data/img/projects/hsr.gif" alt="" class="card-img-top">
            <div class="card-body">
              <h5 class="mb-0">
                Home Service Robot
                <a href="https://github.com/navarrs/robot"> <i class="fab fa-github"></i></a>
              </h5>
              <p class="card-text"> 
                Udacity's Robotics Software Engineer Nanodegree Final Project
              </p>
            </div>
          </div>
        </div>

        <div class="col-lg-4 mb-4">
          <div class="card">
            <img src="data/img/projects/vanttec-vis.png" alt="" class="card-img-top">
            <div class="card-body">
              <h5 class="mb-0">
                VantTec
                <a href="https://github.com/navarrs/cv-roboboat"> <i class="fab fa-github"></i></a>
              </h5>
              <p class="card-text"> 
                Perception system of <a href="https://www.vanttec.mx">VantTec's</a> autonomous boat 
              </p>
            </div>
          </div>
        </div>

      </div>
    </div>

    </section>
    
  </div>

  <!-- Bootstrap core JavaScript -->
  <script src="vendor/jquery/jquery.min.js"></script>
  <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

  <!-- Plugin JavaScript -->
  <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

  <!-- Custom scripts for this template -->
  <script src="js/resume.min.js"></script>

</body>

</html>
